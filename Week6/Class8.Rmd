---
title: "Class8"
author: "Ayse"
date: "2/11/2022"
output:
  pdf_document: default
  html_document: default
---

##Preparing the data
```{r}
#dataset was downloaded and put into the same directory as project file
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```
We need to make a new dataframe with diagnosis column removed, this will be the answer/check to how well our unsupervised analysis works. The removed column can be sotred as a vector

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df[,1]
# for making vector diagnosis <- c(wisc.df$diagnosis) - this turns into a vector of 1s and 2s.
diagnosis
```

    Q1. How many observations are in this dataset? There are 569 observations
    
```{r}
nrow(wisc.df)
```
    
    Q2. How many of the observations have a malignant diagnosis? 212 observations have malignant diagnosis
```{r}
sum(grepl("M", diagnosis))

```
    
    Q3. How many variables/features in the data are suffixed with _mean?
10 variables in the data are suffixed with _mean. I just checked the column headings for this.

##Performing PCA
```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
#pca
wisc.pr <- prcomp(wisc.data, scale. = TRUE)
summary(wisc.pr)
```



    Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
  
 44.27%
  
    Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
 
  3
    
    Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
    
  7

biplot of pca 
```{r}
biplot(wisc.pr)
```

  
    Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
The plot has too many points and is very difficult to understand because there are too many variables to consider.

```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[,1], wisc.pr$x[,2] , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")

```


    Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
  The x axis is more spread out for PC3 compared to PC2 in the plot above. There seems to be less separation between the malignant and benign diagnosed observations in the plot of pc1 and 3 compared to pc1 and 2. this is because pc2 explains more of the variance than pc3.
```{r}
plot( wisc.pr$x[,1], wisc.pr$x[,3] , col = diagnosis , 
     xlab = "PC1", ylab = "PC3")

```

```{r}
# Load the ggplot2 package (it is already installed)
library(ggplot2)

df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

## variance explained
Make scree plots
```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component
pve <- pr.var / (sum(pr.var))

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
#Alternative
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```


    Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
-0.26085376
```{r}
wisc.pr$rotation[,1]
```
    Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
5

```{r}
summary(wisc.pr)
```

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)

data.dist <- dist(data.scaled)

wisc.hclust <- hclust(data.dist, method = "complete")
```


    Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
19

```{r}
plot(wisc.hclust, main = "Complete")+
  abline(h = 19, col="red", lty=2)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 4)
table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
It looks like increasing the number of clusters leads to clusters being identified that are more separated (majority benign or majority malignant), but also clusters with fewer observations. Therefore, I don't think there is a better match than 4.

```{r}
wisc.hclust.clusters.two <- cutree(wisc.hclust, 2)
table(wisc.hclust.clusters.two, diagnosis)
```

Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
For this dataset, the complete method (tried above) seems to give the best results. From single, average, or ward.D2, ward.D2 is the best performing in terms of clustering malignant and benign samples correctly.

```{r}
wisc.hclust.2 <- hclust(data.dist, method = "single")
wisc.hclust.2.clusters <- cutree(wisc.hclust.2, 4)
table(wisc.hclust.2.clusters, diagnosis)

```

```{r}
wisc.hclust.3 <- hclust(data.dist, method = "average")
wisc.hclust.3.clusters <- cutree(wisc.hclust.3, 4)
table(wisc.hclust.3.clusters, diagnosis)
```

```{r}
wisc.hclust.4 <- hclust(data.dist, method = "ward.D2")
wisc.hclust.4.clusters <- cutree(wisc.hclust.4, 4)
table(wisc.hclust.4.clusters, diagnosis)
```

##kmeans clustering
```{r}
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart= 2)
table(wisc.km$clust, diagnosis)
```


    Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?
K-means did better than hclust for separating the two diagnoses.
```{r}
table(wisc.hclust.clusters,wisc.km$clust)
```

##Combining methods
For 90% of variance to be explained, we need 7 PCs.
```{r}
wisc.pr.dist <- dist(wisc.pr$x[,1:7])

wisc.pr.hclust <- hclust(wisc.pr.dist, method = "ward.D2")
plot(wisc.pr.hclust, main = "ward.D2")

```
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
g <- relevel(g,2)
levels(g)
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```
```{r}
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters,diagnosis)
```

    Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.
    kmeans works better.
```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

#sensitivity and specificity


    Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
Kmeans has better sensitivity, but in terms of specificity, kmeans and hclust had similar performance.

Sensitivity: TP/(TP+FN)
Specificity: TN/(TN+FN)
```{r}
sensitivity.km <- 175/(175+37)
sensitivity.km
sensitivity.hc <- 165/(165+47)
sensitivity.hc 

specificity.km <- 343/(343+14)
specificity.km
specificity.hc <- 343/(343+14) 
specificity.hc
```

#Prediction
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
Q18. Which of these new patients should we prioritize for follow up based on your results? The second group, which is predicted to be malignant.
